{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/Users/javie/Desktop/CVC\")\n",
    "\n",
    "import os\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch\n",
    "import trimesh\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from tqdm.notebook import tnrange, tqdm\n",
    "\n",
    "from utils.slices import *\n",
    "from utils.orientaciones import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        pkl_path = \"../data/oriented_dataset/mesh_info2.pkl\"\n",
    "        if os.path.exists(pkl_path):\n",
    "            with open(pkl_path, 'rb') as archivo:\n",
    "                pkl_object = pickle.load(archivo)\n",
    "            self.classes = pkl_object[0]\n",
    "            self.mesh_info = torch.tensor(pkl_object[1])\n",
    "            self.labels = torch.tensor(pkl_object[2]) #index of self.classes\n",
    "\n",
    "        else:\n",
    "            self.classes = os.listdir(root_dir)\n",
    "            self.mesh_info = []\n",
    "            self.labels = []\n",
    "            \n",
    "            for class_index, class_path in enumerate(self.classes):\n",
    "                mesh_paths = os.listdir(root_dir + class_path)\n",
    "                for path in mesh_paths:\n",
    "                    mesh = trimesh.load(root_dir + class_path + '/' + path)\n",
    "                    \n",
    "                    xsec_0 = xsection(mesh, origin_plane=[0,0,0])\n",
    "                    xsec_1_pos = xsection(mesh, origin_plane=[1,0,0])\n",
    "                    xsec_1_neg = xsection(mesh, origin_plane=[-1,0,0])\n",
    "\n",
    "                    xsec_0 = np.column_stack(flip_and_roll(xsec_0[:, 1],xsec_0[:, 2]))\n",
    "                    xsec_1_pos = np.column_stack(flip_and_roll(xsec_1_pos[:, 1],xsec_1_pos[:, 2]))\n",
    "                    xsec_1_neg = np.column_stack(flip_and_roll(xsec_1_neg[:, 1],xsec_1_neg[:, 2]))\n",
    "                    \n",
    "                    xsec_0 = xsec_0[np.linspace(0,xsec_0.shape[0],21,dtype=np.int16)[:-1]]\n",
    "                    xsec_1_pos = xsec_1_pos[np.linspace(0,xsec_1_pos.shape[0],21,dtype=np.int16)[:-1]]\n",
    "                    xsec_1_neg = xsec_1_neg[np.linspace(0,xsec_1_neg.shape[0],21,dtype=np.int16)[:-1]]\n",
    "\n",
    "                    xsec_0 = xsec_0.flatten()\n",
    "                    xsec_1_pos = xsec_1_pos.flatten()\n",
    "                    xsec_1_neg = xsec_1_neg.flatten()\n",
    "                    \n",
    "                    descriptors_list = [mesh.area, mesh.volume, mesh.convex_hull.volume, mesh.volume/mesh.convex_hull.volume]\n",
    "                    descriptors_list.extend(xsec_0)\n",
    "                    descriptors_list.extend(xsec_1_pos)\n",
    "                    descriptors_list.extend(xsec_1_neg)\n",
    "                    self.mesh_info.append(descriptors_list)\n",
    "                    self.labels.append(class_index)\n",
    "            \n",
    "            with open(pkl_path, 'wb') as archivo:\n",
    "                pickle.dump([self.classes, self.mesh_info, self.labels], archivo)\n",
    "                \n",
    "            #self.classes = torch.tensor(self.classes)\n",
    "            self.mesh_info = torch.tensor(self.mesh_info)\n",
    "            self.labels = torch.tensor(self.labels)\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.mesh_info[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\javie\\AppData\\Local\\Temp\\ipykernel_20832\\3138988362.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mesh_info = torch.tensor(pkl_object[1])\n",
      "C:\\Users\\javie\\AppData\\Local\\Temp\\ipykernel_20832\\3138988362.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(pkl_object[2]) #index of self.classes\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomImageDataset(\"../data/oriented_dataset/Barley STL files/Dundee/\")\n",
    "\n",
    "train_size = int(0.8 * len(dataset))  # 80% de datos de entrenamiento\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([16, 124])\n",
      "Shape of y: torch.Size([16]) torch.int64\n",
      "Shape of X [N, C, H, W]: torch.Size([16, 124])\n",
      "Shape of y: torch.Size([16]) torch.int64\n",
      "Shape of X [N, C, H, W]: torch.Size([16, 124])\n",
      "Shape of y: torch.Size([16]) torch.int64\n",
      "Shape of X [N, C, H, W]: torch.Size([16, 124])\n",
      "Shape of y: torch.Size([16]) torch.int64\n",
      "Shape of X [N, C, H, W]: torch.Size([16, 124])\n",
      "Shape of y: torch.Size([16]) torch.int64\n",
      "Shape of X [N, C, H, W]: torch.Size([16, 124])\n",
      "Shape of y: torch.Size([16]) torch.int64\n",
      "Shape of X [N, C, H, W]: torch.Size([16, 124])\n",
      "Shape of y: torch.Size([16]) torch.int64\n",
      "Shape of X [N, C, H, W]: torch.Size([16, 124])\n",
      "Shape of y: torch.Size([16]) torch.int64\n",
      "Shape of X [N, C, H, W]: torch.Size([16, 124])\n",
      "Shape of y: torch.Size([16]) torch.int64\n",
      "Shape of X [N, C, H, W]: torch.Size([16, 124])\n",
      "Shape of y: torch.Size([16]) torch.int64\n",
      "Shape of X [N, C, H, W]: torch.Size([16, 124])\n",
      "Shape of y: torch.Size([16]) torch.int64\n",
      "Shape of X [N, C, H, W]: torch.Size([16, 124])\n",
      "Shape of y: torch.Size([16]) torch.int64\n",
      "Shape of X [N, C, H, W]: torch.Size([16, 124])\n",
      "Shape of y: torch.Size([16]) torch.int64\n",
      "Shape of X [N, C, H, W]: torch.Size([16, 124])\n",
      "Shape of y: torch.Size([16]) torch.int64\n",
      "Shape of X [N, C, H, W]: torch.Size([16, 124])\n",
      "Shape of y: torch.Size([16]) torch.int64\n",
      "Shape of X [N, C, H, W]: torch.Size([16, 124])\n",
      "Shape of y: torch.Size([16]) torch.int64\n",
      "Shape of X [N, C, H, W]: torch.Size([16, 124])\n",
      "Shape of y: torch.Size([16]) torch.int64\n",
      "Shape of X [N, C, H, W]: torch.Size([2, 124])\n",
      "Shape of y: torch.Size([2]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_loader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GrainClassifier(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=124, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (classification_layer): Linear(in_features=512, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "'''# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "'''\n",
    "# Define model\n",
    "class GrainClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(124, 512, dtype=torch.double),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512, dtype=torch.double),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512, dtype=torch.double),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512, dtype=torch.double),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.classification_layer = nn.Linear(512, 7, dtype=torch.double)\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        x = self.classification_layer(x)\n",
    "        return x\n",
    "\n",
    "'''model = GrainClassifier().to(device)\n",
    "print(model)'''\n",
    "\n",
    "model = GrainClassifier()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X = torch.rand(10, 124, device=device)\\nX *= -1\\nlogits = model(X)\\npred_probab = nn.Softmax(dim=1)(logits)\\ny_pred = pred_probab.argmax(1)\\nprint(f\"Input: {X}\")\\nprint(f\"Prob class: {pred_probab}\")\\nprint(f\"Predicted class: {y_pred}\")'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    X = torch.rand(10, 124, device=device)\n",
    "    X *= -1\n",
    "    logits = model(X)\n",
    "    pred_probab = nn.Softmax(dim=1)(logits)\n",
    "    y_pred = pred_probab.argmax(1)\n",
    "    print(f\"Input: {X}\")\n",
    "    print(f\"Prob class: {pred_probab}\")\n",
    "    print(f\"Predicted class: {y_pred}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers specified in the torch.optim package\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-3\n",
    "momentum = 0.9\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloader):\n",
    "    running_loss = 0.\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "    #print(\"Total loss: \", running_loss)\n",
    "\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(dataloader):\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predictions = torch.max(outputs.data, 1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)  # Número total de ejemplos\n",
    "\n",
    "    # Calcula la precisión como el número de predicciones correctas dividido por el total\n",
    "    accuracy = correct / total\n",
    "        \n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  , train loss: 33.13236400123571, test loss: 8.644609027301673 , accuracy: 0.2608695652173913\n",
      "Epoch: 25 , train loss: 29.249603168125695, test loss: 8.209316002115175 , accuracy: 0.2608695652173913\n",
      "Epoch: 50 , train loss: 28.209683565548275, test loss: 8.533362828311159 , accuracy: 0.2028985507246377\n",
      "Epoch: 75 , train loss: 27.58757585006918, test loss: 8.5207140787995 , accuracy: 0.21739130434782608\n",
      "Epoch: 100, train loss: 27.49423286165683, test loss: 8.837730228069466 , accuracy: 0.21739130434782608\n",
      "Epoch: 125, train loss: 27.1392648690767, test loss: 8.558119150565378 , accuracy: 0.2318840579710145\n",
      "Epoch: 150, train loss: 26.902616886355396, test loss: 8.49139460376711 , accuracy: 0.2318840579710145\n",
      "Epoch: 175, train loss: 26.189493173109966, test loss: 8.80580346272503 , accuracy: 0.2028985507246377\n",
      "Epoch: 200, train loss: 27.132250019494045, test loss: 9.037732679339989 , accuracy: 0.2318840579710145\n",
      "Epoch: 225, train loss: 26.839565155963975, test loss: 8.588514356774583 , accuracy: 0.2608695652173913\n",
      "Epoch: 250, train loss: 26.831743405061975, test loss: 8.97701075858628 , accuracy: 0.2028985507246377\n",
      "Epoch: 275, train loss: 26.108534498938457, test loss: 8.61837340341198 , accuracy: 0.2753623188405797\n",
      "Epoch: 300, train loss: 25.80758302952456, test loss: 8.175432209660597 , accuracy: 0.3333333333333333\n",
      "Epoch: 325, train loss: 25.82705202667628, test loss: 8.174407706543287 , accuracy: 0.36231884057971014\n",
      "Epoch: 350, train loss: 26.478564840416187, test loss: 7.989342366236925 , accuracy: 0.3333333333333333\n",
      "Epoch: 375, train loss: 25.52410366362734, test loss: 8.850337131026937 , accuracy: 0.3188405797101449\n",
      "Epoch: 400, train loss: 26.79443960782317, test loss: 9.101241239339277 , accuracy: 0.2028985507246377\n",
      "Epoch: 425, train loss: 24.9573729524749, test loss: 9.203269925412371 , accuracy: 0.30434782608695654\n",
      "Epoch: 450, train loss: 26.63570839208471, test loss: 8.085162090898805 , accuracy: 0.36231884057971014\n",
      "Epoch: 475, train loss: 26.074117661983372, test loss: 8.687399475600673 , accuracy: 0.2898550724637681\n",
      "Epoch: 500, train loss: 25.572550804558404, test loss: 8.347968778754893 , accuracy: 0.36231884057971014\n",
      "Epoch: 525, train loss: 25.690049678545204, test loss: 9.751305780350663 , accuracy: 0.2753623188405797\n",
      "Epoch: 550, train loss: 25.105660327623482, test loss: 8.111588363004083 , accuracy: 0.37681159420289856\n",
      "Epoch: 575, train loss: 24.404613981883113, test loss: 8.648586491036177 , accuracy: 0.36231884057971014\n",
      "Epoch: 600, train loss: 24.318911114612266, test loss: 8.312048444574538 , accuracy: 0.36231884057971014\n",
      "Epoch: 625, train loss: 25.017509269033287, test loss: 8.803333703362002 , accuracy: 0.34782608695652173\n",
      "Epoch: 650, train loss: 26.012614914374247, test loss: 7.932391503998451 , accuracy: 0.36231884057971014\n",
      "Epoch: 675, train loss: 24.67168581108507, test loss: 8.98265643291241 , accuracy: 0.3188405797101449\n",
      "Epoch: 700, train loss: 24.999669489242073, test loss: 8.937968272561246 , accuracy: 0.36231884057971014\n",
      "Epoch: 725, train loss: 24.198339635511157, test loss: 9.684174098106848 , accuracy: 0.3188405797101449\n",
      "Epoch: 750, train loss: 23.67350946663805, test loss: 8.117932251780474 , accuracy: 0.4057971014492754\n",
      "Epoch: 775, train loss: 23.746196798280945, test loss: 8.505586304179879 , accuracy: 0.37681159420289856\n",
      "Epoch: 800, train loss: 25.07935040941016, test loss: 8.3140692313001 , accuracy: 0.37681159420289856\n",
      "Epoch: 825, train loss: 23.120190959742065, test loss: 8.39455226307901 , accuracy: 0.37681159420289856\n",
      "Epoch: 850, train loss: 24.640833117848267, test loss: 9.395050960122573 , accuracy: 0.3333333333333333\n",
      "Epoch: 875, train loss: 22.415250958265098, test loss: 9.88887073678982 , accuracy: 0.3333333333333333\n",
      "Epoch: 900, train loss: 24.66745869189949, test loss: 7.924087261251562 , accuracy: 0.43478260869565216\n",
      "Epoch: 925, train loss: 23.955727105097186, test loss: 8.367932362604655 , accuracy: 0.42028985507246375\n",
      "Epoch: 950, train loss: 25.04502351177202, test loss: 8.531362785482468 , accuracy: 0.391304347826087\n",
      "Epoch: 975, train loss: 23.554045023583623, test loss: 8.40837518695185 , accuracy: 0.43478260869565216\n",
      "Epoch: 1000, train loss: 23.93193315487725, test loss: 8.455731632706641 , accuracy: 0.36231884057971014\n",
      "Epoch: 1025, train loss: 23.78334780181389, test loss: 9.1875778474801 , accuracy: 0.3333333333333333\n",
      "Epoch: 1050, train loss: 24.53742028596349, test loss: 9.615381174809242 , accuracy: 0.30434782608695654\n",
      "Epoch: 1075, train loss: 23.832085716321824, test loss: 8.890415156868205 , accuracy: 0.37681159420289856\n",
      "Epoch: 1100, train loss: 23.249900152792502, test loss: 8.218597420915561 , accuracy: 0.42028985507246375\n",
      "Epoch: 1125, train loss: 23.208212092768363, test loss: 8.39010692376456 , accuracy: 0.37681159420289856\n",
      "Epoch: 1150, train loss: 24.45544179288074, test loss: 8.467829196821812 , accuracy: 0.3333333333333333\n",
      "Epoch: 1175, train loss: 23.918323088625044, test loss: 8.255473697319175 , accuracy: 0.43478260869565216\n",
      "Epoch: 1200, train loss: 23.044222610114712, test loss: 9.117980205009914 , accuracy: 0.36231884057971014\n",
      "Epoch: 1225, train loss: 23.296935080704902, test loss: 8.957885846637097 , accuracy: 0.36231884057971014\n",
      "Epoch: 1250, train loss: 22.21117057176193, test loss: 9.218121529080085 , accuracy: 0.36231884057971014\n",
      "Epoch: 1275, train loss: 21.96560824687645, test loss: 9.102164589881514 , accuracy: 0.4057971014492754\n",
      "Epoch: 1300, train loss: 23.192851601404396, test loss: 7.939246324350718 , accuracy: 0.391304347826087\n",
      "Epoch: 1325, train loss: 22.76004350199358, test loss: 9.320833619684404 , accuracy: 0.3188405797101449\n",
      "Epoch: 1350, train loss: 22.172195090527424, test loss: 8.426725484902118 , accuracy: 0.391304347826087\n",
      "Epoch: 1375, train loss: 22.372526835951952, test loss: 8.232752895882408 , accuracy: 0.36231884057971014\n",
      "Epoch: 1400, train loss: 20.832862465218295, test loss: 8.510426365607497 , accuracy: 0.4057971014492754\n",
      "Epoch: 1425, train loss: 22.55189841098744, test loss: 9.697732356259504 , accuracy: 0.36231884057971014\n",
      "Epoch: 1450, train loss: 21.62339699214645, test loss: 8.469685160361168 , accuracy: 0.34782608695652173\n",
      "Epoch: 1475, train loss: 22.573406576547683, test loss: 8.58762528189012 , accuracy: 0.4057971014492754\n",
      "Epoch: 1500, train loss: 24.42642081362705, test loss: 8.96851906545866 , accuracy: 0.3333333333333333\n",
      "Epoch: 1525, train loss: 22.135782937910133, test loss: 9.943365915042559 , accuracy: 0.3188405797101449\n",
      "Epoch: 1550, train loss: 20.667944751650975, test loss: 9.160453678371736 , accuracy: 0.391304347826087\n",
      "Epoch: 1575, train loss: 21.81702757234538, test loss: 8.854982865509818 , accuracy: 0.4057971014492754\n",
      "Epoch: 1600, train loss: 20.42585937768764, test loss: 8.563417310933112 , accuracy: 0.391304347826087\n",
      "Epoch: 1625, train loss: 21.257728835740643, test loss: 10.342064927408112 , accuracy: 0.3333333333333333\n",
      "Epoch: 1650, train loss: 21.7627430355381, test loss: 9.537007978217849 , accuracy: 0.3188405797101449\n",
      "Epoch: 1675, train loss: 19.249214766190192, test loss: 9.876869609431381 , accuracy: 0.391304347826087\n",
      "Epoch: 1700, train loss: 20.542582188588614, test loss: 9.902554143543554 , accuracy: 0.391304347826087\n",
      "Epoch: 1725, train loss: 20.2645896322696, test loss: 8.865516836831409 , accuracy: 0.37681159420289856\n",
      "Epoch: 1750, train loss: 19.368687271037086, test loss: 9.755199843737879 , accuracy: 0.391304347826087\n",
      "Epoch: 1775, train loss: 20.542877364098413, test loss: 9.65020601895201 , accuracy: 0.4057971014492754\n",
      "Epoch: 1800, train loss: 19.561013185439904, test loss: 10.541416598118174 , accuracy: 0.3333333333333333\n",
      "Epoch: 1825, train loss: 20.695722232405707, test loss: 9.195367709692974 , accuracy: 0.42028985507246375\n",
      "Epoch: 1850, train loss: 20.123453370538048, test loss: 9.671017873924244 , accuracy: 0.2898550724637681\n",
      "Epoch: 1875, train loss: 20.573823244355037, test loss: 11.166548048576711 , accuracy: 0.30434782608695654\n",
      "Epoch: 1900, train loss: 20.062260596856653, test loss: 9.170100710249736 , accuracy: 0.4057971014492754\n",
      "Epoch: 1925, train loss: 17.903513858362757, test loss: 10.761424210191754 , accuracy: 0.37681159420289856\n",
      "Epoch: 1950, train loss: 18.04636032289288, test loss: 9.174308892413318 , accuracy: 0.4057971014492754\n",
      "Epoch: 1975, train loss: 20.35264782223484, test loss: 10.671418459280758 , accuracy: 0.3188405797101449\n",
      "Epoch: 2000, train loss: 19.589884191390578, test loss: 9.52740896809458 , accuracy: 0.37681159420289856\n",
      "Epoch: 2025, train loss: 19.2552527082104, test loss: 10.836881928477382 , accuracy: 0.36231884057971014\n",
      "Epoch: 2050, train loss: 19.649636955141094, test loss: 10.338749608683315 , accuracy: 0.4057971014492754\n",
      "Epoch: 2075, train loss: 18.349007911321706, test loss: 9.488512687820933 , accuracy: 0.37681159420289856\n",
      "Epoch: 2100, train loss: 20.223885081407136, test loss: 9.472227737903856 , accuracy: 0.4057971014492754\n",
      "Epoch: 2125, train loss: 19.96327090733538, test loss: 9.493719857425889 , accuracy: 0.4057971014492754\n",
      "Epoch: 2150, train loss: 17.918382890909974, test loss: 10.735258680492135 , accuracy: 0.3188405797101449\n",
      "Epoch: 2175, train loss: 20.223171286321065, test loss: 9.497132977225942 , accuracy: 0.4057971014492754\n",
      "Epoch: 2200, train loss: 17.55907452930506, test loss: 9.741166523983 , accuracy: 0.42028985507246375\n",
      "Epoch: 2225, train loss: 17.214493075930122, test loss: 9.566211582948783 , accuracy: 0.37681159420289856\n",
      "Epoch: 2250, train loss: 17.142500659852985, test loss: 9.466013251750372 , accuracy: 0.4057971014492754\n",
      "Epoch: 2275, train loss: 18.08398487408617, test loss: 10.24506015842117 , accuracy: 0.42028985507246375\n",
      "Epoch: 2300, train loss: 17.716985866901595, test loss: 10.19770301029505 , accuracy: 0.42028985507246375\n",
      "Epoch: 2325, train loss: 18.42153081692261, test loss: 9.052446801090307 , accuracy: 0.36231884057971014\n",
      "Epoch: 2350, train loss: 19.263614647252997, test loss: 10.345574655023594 , accuracy: 0.4492753623188406\n",
      "Epoch: 2375, train loss: 17.64241494630931, test loss: 9.465059334902342 , accuracy: 0.391304347826087\n",
      "Epoch: 2400, train loss: 18.345917759415958, test loss: 11.444554365450378 , accuracy: 0.37681159420289856\n",
      "Epoch: 2425, train loss: 18.734859468794724, test loss: 9.852247854445007 , accuracy: 0.34782608695652173\n",
      "Epoch: 2450, train loss: 20.968120888406204, test loss: 11.253956420096475 , accuracy: 0.2753623188405797\n",
      "Epoch: 2475, train loss: 18.249010432891666, test loss: 9.539483111508344 , accuracy: 0.42028985507246375\n",
      "Epoch: 2500, train loss: 16.414086400013367, test loss: 12.520335183392657 , accuracy: 0.34782608695652173\n",
      "Epoch: 2525, train loss: 16.992930349676502, test loss: 10.915154134075532 , accuracy: 0.391304347826087\n",
      "Epoch: 2550, train loss: 16.401037745985928, test loss: 10.701663097825193 , accuracy: 0.36231884057971014\n",
      "Epoch: 2575, train loss: 19.553593320588625, test loss: 10.548566248688827 , accuracy: 0.4492753623188406\n",
      "Epoch: 2600, train loss: 21.60726087088236, test loss: 16.481225196407827 , accuracy: 0.2463768115942029\n",
      "Epoch: 2625, train loss: 15.999405551756093, test loss: 11.825546136476333 , accuracy: 0.37681159420289856\n",
      "Epoch: 2650, train loss: 17.11941786144454, test loss: 9.88980900078068 , accuracy: 0.42028985507246375\n",
      "Epoch: 2675, train loss: 17.17370427883958, test loss: 14.375902421977692 , accuracy: 0.34782608695652173\n",
      "Epoch: 2700, train loss: 19.232927459209918, test loss: 12.521553637992195 , accuracy: 0.3188405797101449\n",
      "Epoch: 2725, train loss: 18.78019427916095, test loss: 10.170474328822642 , accuracy: 0.36231884057971014\n",
      "Epoch: 2750, train loss: 15.80882400894744, test loss: 11.443992757626393 , accuracy: 0.37681159420289856\n",
      "Epoch: 2775, train loss: 14.889367967669969, test loss: 12.18421291833514 , accuracy: 0.4057971014492754\n",
      "Epoch: 2800, train loss: 13.061905037020182, test loss: 12.313230985013856 , accuracy: 0.37681159420289856\n",
      "Epoch: 2825, train loss: 16.62836336102757, test loss: 10.387356986483493 , accuracy: 0.43478260869565216\n",
      "Epoch: 2850, train loss: 15.284666766366394, test loss: 13.651940629590383 , accuracy: 0.36231884057971014\n",
      "Epoch: 2875, train loss: 15.975308023444503, test loss: 10.447001180868686 , accuracy: 0.42028985507246375\n",
      "Epoch: 2900, train loss: 14.535351506771965, test loss: 10.940971431692661 , accuracy: 0.42028985507246375\n",
      "Epoch: 2925, train loss: 21.6078470565219, test loss: 9.98229827193888 , accuracy: 0.3333333333333333\n",
      "Epoch: 2950, train loss: 17.49472876098128, test loss: 13.521484710998765 , accuracy: 0.4057971014492754\n",
      "Epoch: 2975, train loss: 15.683286015112262, test loss: 12.754932368228573 , accuracy: 0.36231884057971014\n",
      "Epoch: 3000, train loss: 33.25656875996246, test loss: 10.591880493102941 , accuracy: 0.391304347826087\n",
      "Epoch: 3025, train loss: 13.886019746884216, test loss: 11.682669588700756 , accuracy: 0.42028985507246375\n",
      "Epoch: 3050, train loss: 14.394125383841546, test loss: 12.545635722724182 , accuracy: 0.43478260869565216\n",
      "Epoch: 3075, train loss: 20.51355087277911, test loss: 10.783426759955004 , accuracy: 0.4057971014492754\n",
      "Epoch: 3100, train loss: 20.361808851829302, test loss: 11.551768708365545 , accuracy: 0.42028985507246375\n",
      "Epoch: 3125, train loss: 16.994309968787515, test loss: 10.57857417632033 , accuracy: 0.4057971014492754\n",
      "Epoch: 3150, train loss: 12.948660998846508, test loss: 13.615451816955721 , accuracy: 0.34782608695652173\n",
      "Epoch: 3175, train loss: 14.485457870668863, test loss: 11.157123918915065 , accuracy: 0.43478260869565216\n",
      "Epoch: 3200, train loss: 17.79314002689065, test loss: 12.040075536922908 , accuracy: 0.43478260869565216\n",
      "Epoch: 3225, train loss: 19.96845581729533, test loss: 11.176190352931494 , accuracy: 0.37681159420289856\n",
      "Epoch: 3250, train loss: 27.64986843133069, test loss: 12.812123791940998 , accuracy: 0.36231884057971014\n",
      "Epoch: 3275, train loss: 14.991001517769352, test loss: 15.553883457906192 , accuracy: 0.4057971014492754\n",
      "Epoch: 3300, train loss: 22.6003763651855, test loss: 10.139914627502018 , accuracy: 0.34782608695652173\n",
      "Epoch: 3325, train loss: 13.977658209647808, test loss: 14.082271334459339 , accuracy: 0.391304347826087\n",
      "Epoch: 3350, train loss: 13.72935856918081, test loss: 11.549354326661671 , accuracy: 0.4057971014492754\n",
      "Epoch: 3375, train loss: 19.38415003396941, test loss: 16.55549416713001 , accuracy: 0.2318840579710145\n",
      "Epoch: 3400, train loss: 22.89596039589915, test loss: 18.40771052159561 , accuracy: 0.2608695652173913\n",
      "Epoch: 3425, train loss: 12.825019018454844, test loss: 11.760078924715536 , accuracy: 0.391304347826087\n",
      "Epoch: 3450, train loss: 16.30397281502706, test loss: 11.700698613470859 , accuracy: 0.43478260869565216\n",
      "Epoch: 3475, train loss: 21.152229777992513, test loss: 8.710736852271946 , accuracy: 0.391304347826087\n",
      "Epoch: 3500, train loss: 15.40157715053882, test loss: 10.307148693882958 , accuracy: 0.4782608695652174\n",
      "Epoch: 3525, train loss: 15.682347810847126, test loss: 10.84210894205887 , accuracy: 0.4057971014492754\n",
      "Epoch: 3550, train loss: 16.813359203752, test loss: 10.678191993687346 , accuracy: 0.43478260869565216\n",
      "Epoch: 3575, train loss: 21.81673586945302, test loss: 11.76618733728425 , accuracy: 0.3188405797101449\n",
      "Epoch: 3600, train loss: 22.581857182743295, test loss: 11.630525523071388 , accuracy: 0.4057971014492754\n",
      "Epoch: 3625, train loss: 16.35695406005768, test loss: 15.60439975269426 , accuracy: 0.36231884057971014\n",
      "Epoch: 3650, train loss: 21.182775461280386, test loss: 13.616559726385674 , accuracy: 0.30434782608695654\n",
      "Epoch: 3675, train loss: 11.947593222501041, test loss: 13.709463333047228 , accuracy: 0.43478260869565216\n",
      "Epoch: 3700, train loss: 20.927168960253532, test loss: 9.214737702385818 , accuracy: 0.42028985507246375\n",
      "Epoch: 3725, train loss: 26.274805489054927, test loss: 8.832693421108067 , accuracy: 0.42028985507246375\n",
      "Epoch: 3750, train loss: 20.646340036245437, test loss: 15.12735033922943 , accuracy: 0.34782608695652173\n",
      "Epoch: 3775, train loss: 15.063022846334768, test loss: 12.704022017622965 , accuracy: 0.4057971014492754\n",
      "Epoch: 3800, train loss: 13.309444843419378, test loss: 13.87691226958706 , accuracy: 0.36231884057971014\n",
      "Epoch: 3825, train loss: 16.381350016888337, test loss: 13.481867794713052 , accuracy: 0.3333333333333333\n",
      "Epoch: 3850, train loss: 20.43789776156171, test loss: 13.82879373988807 , accuracy: 0.34782608695652173\n",
      "Epoch: 3875, train loss: 19.40012138700761, test loss: 10.74165778321144 , accuracy: 0.42028985507246375\n",
      "Epoch: 3900, train loss: 11.082555858832066, test loss: 15.166440314308735 , accuracy: 0.3333333333333333\n",
      "Epoch: 3925, train loss: 18.139885852134572, test loss: 11.696704220354096 , accuracy: 0.42028985507246375\n",
      "Epoch: 3950, train loss: 15.591507583609951, test loss: 12.08887946001921 , accuracy: 0.4057971014492754\n",
      "Epoch: 3975, train loss: 19.053225913699922, test loss: 15.586764890767022 , accuracy: 0.36231884057971014\n",
      "Epoch: 4000, train loss: 22.589556548304166, test loss: 7.862129443904271 , accuracy: 0.42028985507246375\n",
      "Epoch: 4025, train loss: 20.309398678485152, test loss: 11.121130815741596 , accuracy: 0.30434782608695654\n",
      "Epoch: 4050, train loss: 15.410811651525965, test loss: 9.216704304092309 , accuracy: 0.4782608695652174\n",
      "Epoch: 4075, train loss: 19.76990490114721, test loss: 10.78333240622525 , accuracy: 0.34782608695652173\n",
      "Epoch: 4100, train loss: 18.040172333487785, test loss: 12.579714817114649 , accuracy: 0.3333333333333333\n",
      "Epoch: 4125, train loss: 15.8973302640165, test loss: 12.68800958939742 , accuracy: 0.43478260869565216\n",
      "Epoch: 4150, train loss: 20.009561936855377, test loss: 12.068081552434348 , accuracy: 0.36231884057971014\n",
      "Epoch: 4175, train loss: 16.721348701796156, test loss: 12.813597274820948 , accuracy: 0.4492753623188406\n",
      "Epoch: 4200, train loss: 16.500528316389374, test loss: 11.611306071383632 , accuracy: 0.4057971014492754\n",
      "Epoch: 4225, train loss: 23.834895276115144, test loss: 10.963019199380179 , accuracy: 0.34782608695652173\n",
      "Epoch: 4250, train loss: 16.822849483296515, test loss: 11.241618827564668 , accuracy: 0.4057971014492754\n",
      "Epoch: 4275, train loss: 18.913472246728567, test loss: 12.031119319416 , accuracy: 0.3188405797101449\n",
      "Epoch: 4300, train loss: 13.38880863984189, test loss: 12.233014872937233 , accuracy: 0.42028985507246375\n",
      "Epoch: 4325, train loss: 11.832348228068694, test loss: 12.439402654215659 , accuracy: 0.42028985507246375\n",
      "Epoch: 4350, train loss: 21.30697401390223, test loss: 11.140935735814544 , accuracy: 0.37681159420289856\n",
      "Epoch: 4375, train loss: 14.836729668254025, test loss: 14.43881170119296 , accuracy: 0.2753623188405797\n",
      "Epoch: 4400, train loss: 21.188660277949243, test loss: 8.639340794188584 , accuracy: 0.43478260869565216\n",
      "Epoch: 4425, train loss: 22.521806758290417, test loss: 8.744358967565466 , accuracy: 0.36231884057971014\n",
      "Epoch: 4450, train loss: 18.111591567854568, test loss: 11.893821780203595 , accuracy: 0.42028985507246375\n",
      "Epoch: 4475, train loss: 23.855939803842713, test loss: 11.91923601828747 , accuracy: 0.43478260869565216\n",
      "Epoch: 4500, train loss: 21.182406971033583, test loss: 9.276280142328744 , accuracy: 0.42028985507246375\n",
      "Epoch: 4525, train loss: 19.87745694058647, test loss: 11.695761184145894 , accuracy: 0.36231884057971014\n",
      "Epoch: 4550, train loss: 22.32381844580667, test loss: 8.192503054618356 , accuracy: 0.463768115942029\n",
      "Epoch: 4575, train loss: 20.194865818676497, test loss: 8.159127181490904 , accuracy: 0.4492753623188406\n",
      "Epoch: 4600, train loss: 19.925777036521886, test loss: 8.642099426369484 , accuracy: 0.4057971014492754\n",
      "Epoch: 4625, train loss: 19.36951129446529, test loss: 9.451076267642708 , accuracy: 0.4057971014492754\n",
      "Epoch: 4650, train loss: 20.016824431912422, test loss: 8.987958724435542 , accuracy: 0.463768115942029\n",
      "Epoch: 4675, train loss: 17.910217821068457, test loss: 11.172766600757205 , accuracy: 0.36231884057971014\n",
      "Epoch: 4700, train loss: 17.907010322723984, test loss: 9.523245661113144 , accuracy: 0.4492753623188406\n",
      "Epoch: 4725, train loss: 21.201213531267367, test loss: 10.749409192203485 , accuracy: 0.42028985507246375\n",
      "Epoch: 4750, train loss: 15.947451009475747, test loss: 10.942647960595837 , accuracy: 0.4782608695652174\n",
      "Epoch: 4775, train loss: 15.220380552700048, test loss: 9.524900167393477 , accuracy: 0.4927536231884058\n",
      "Epoch: 4800, train loss: 17.152047069202855, test loss: 10.254073069494897 , accuracy: 0.42028985507246375\n",
      "Epoch: 4825, train loss: 20.447210823595523, test loss: 9.919573351663132 , accuracy: 0.4927536231884058\n",
      "Epoch: 4850, train loss: 29.05129966680065, test loss: 8.339496332604618 , accuracy: 0.34782608695652173\n",
      "Epoch: 4875, train loss: 19.13773355140024, test loss: 10.087977616062428 , accuracy: 0.3188405797101449\n",
      "Epoch: 4900, train loss: 30.260871276811848, test loss: 7.598948764059889 , accuracy: 0.43478260869565216\n",
      "Epoch: 4925, train loss: 25.866204281743894, test loss: 9.514441008786802 , accuracy: 0.3188405797101449\n",
      "Epoch: 4950, train loss: 21.961291473820115, test loss: 11.70268611454355 , accuracy: 0.4492753623188406\n",
      "Epoch: 4975, train loss: 19.256832450370563, test loss: 10.165969476471721 , accuracy: 0.37681159420289856\n",
      "0.5217391304347826\n"
     ]
    }
   ],
   "source": [
    "max_accuracy = 0\n",
    "for i in range(5000):\n",
    "    train_loss = train_one_epoch(train_loader)\n",
    "    test_loss, accuracy = test_model(test_loader)\n",
    "    if max_accuracy < accuracy:\n",
    "        max_accuracy = accuracy\n",
    "    if i % 25 == 0:\n",
    "        print(\"Epoch: \" + \"%-3i\" % i + \", train loss: \" + str(train_loss) + \", test loss: \" + str(test_loss), \", accuracy: \" + str(accuracy))\n",
    "print(max_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor inputs, labels in train_loader:\\n    preds = model(inputs)\\n    print(\"outputs: \", torch.argmax(preds, dim=1))\\n    print(\"labels: \", labels)\\n    '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for inputs, labels in train_loader:\n",
    "    preds = model(inputs)\n",
    "    print(\"outputs: \", torch.argmax(preds, dim=1))\n",
    "    print(\"labels: \", labels)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5217391304347826\n"
     ]
    }
   ],
   "source": [
    "print(max_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cebada_old_school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
